{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "\n",
    "SPLIT_RATIO = 0.8  # holdout ratio (according to Pareto principle)\n",
    "ITERATIONS = 10  # iterations for unsupervised NB\n",
    "EPSILON = 10**(-6)\n",
    "\n",
    "DATASET1 = '2018S1-proj1_data/breast-cancer-dos.csv'\n",
    "DATASET2 = '2018S1-proj1_data/car-dos.csv'\n",
    "DATASET3 = '2018S1-proj1_data/hypothyroid-dos.csv'\n",
    "DATASET4 = '2018S1-proj1_data/mushroom-dos.csv'\n",
    "DATASETS = [DATASET1, DATASET2, DATASET3, DATASET4]\n",
    "SAMPLE = '2018S1-proj1_data/sample.csv'  # example dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function to create dictionary key given a string\n",
    "@param lst = list of string to combined\n",
    "@return dictionary key for probability (i.e. 0|mild|flu = (0==mild given flu))\n",
    "'''\n",
    "def createKey(lst):\n",
    "    return '|'.join(str(i) for i in lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Random generator\n",
    "@param length = length of the random array\n",
    "@return array containing random number that sums to 1\n",
    "'''\n",
    "def randDistGen(length):\n",
    "    dist = np.random.random(length)\n",
    "    return dist / dist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocessing for supervised to split the data into a training test split\n",
    "@param data = dataset\n",
    "@param flag = True = no split, False = split\n",
    "'''\n",
    "def preprocessSup(data, flag=False):\n",
    "    dataFrame = pd.read_csv(data, header = None)\n",
    "    \n",
    "    if (flag == False):\n",
    "        # Split according to the splitting ratio\n",
    "        split = np.random.rand(len(dataFrame)) < SPLIT_RATIO\n",
    "    \n",
    "        train = dataFrame[split]\n",
    "        test = dataFrame[~split]\n",
    "    else:\n",
    "        train= dataFrame\n",
    "        test= dataFrame\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create supervised Naive Bayes model by returning prior and posterior probability\n",
    "@param trainSet = data that are used for training to generate model\n",
    "@return priobProb, posteriorProb = probability counter\n",
    "'''\n",
    "def trainSup(trainSet):\n",
    "    priorCounts = trainSet.iloc[:,-1].value_counts()\n",
    "    priorProb = priorCounts / trainSet.shape[0]\n",
    "    attribCount = trainSet.shape[1]\n",
    "    posteriorProb = {}\n",
    "\n",
    "    # Iterating over all columns except for the class column\n",
    "    for attrib in range(attribCount - 1):  \n",
    "        # Generate list of unique attribute values and disregard ?\n",
    "        attribValues = list(trainSet[attrib].unique())\n",
    "        if ('?' in attribValues): attribValues.remove('?')\n",
    "\n",
    "        # Calculate posterior probabilities\n",
    "        for c in priorCounts.index:\n",
    "            for val in attribValues:\n",
    "                # first filter by class then by attribute value\n",
    "                filterClass = trainSet[trainSet.iloc[:,-1] == c]\n",
    "                filterClassVal = filterClass[filterClass[attrib] == val]\n",
    "                \n",
    "                # Generate key for dictionary (0|severe|flu means (column 0=severe|flu)\n",
    "                key = createKey([attrib, val, c])\n",
    "                posteriorProb[key] = filterClassVal.shape[0] / priorCounts[c]\n",
    "        \n",
    "    # Iterate every element in dictionary and perform epsilon smoothing\n",
    "    for key, value in posteriorProb.items():\n",
    "        if (value == 0):\n",
    "            posteriorProb[key] = EPSILON\n",
    "    \n",
    "    return priorProb, posteriorProb\n",
    "# (train, test) = preprocessSup(SAMPLE, True)\n",
    "# (prior, posterior) = trainSup(train)\n",
    "# print(prior)\n",
    "# print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Flu', 'Cold', 'Flu', 'Cold', 'Flu']\n",
      "        0       1       2    3     4\n",
      "0  severe    mild    high  yes   Flu\n",
      "1      no  severe  normal  yes  Cold\n",
      "2    mild    mild  normal  yes   Flu\n",
      "3    mild      no  normal   no  Cold\n",
      "4  severe  severe  normal  yes   Flu\n",
      "        0       1       2    3     4\n",
      "0  severe    mild    high  yes   Flu\n",
      "1      no  severe  normal  yes  Cold\n",
      "2    mild    mild  normal  yes   Flu\n",
      "3    mild      no  normal   no  Cold\n",
      "4  severe  severe  normal  yes   Flu\n",
      "Flu     0.6\n",
      "Cold    0.4\n",
      "Name: 4, dtype: float64\n",
      "{'0|severe|Flu': 0.66666666666666663, '0|no|Flu': 1e-06, '0|mild|Flu': 0.33333333333333331, '0|severe|Cold': 1e-06, '0|no|Cold': 0.5, '0|mild|Cold': 0.5, '1|mild|Flu': 0.66666666666666663, '1|severe|Flu': 0.33333333333333331, '1|no|Flu': 1e-06, '1|mild|Cold': 1e-06, '1|severe|Cold': 0.5, '1|no|Cold': 0.5, '2|high|Flu': 0.33333333333333331, '2|normal|Flu': 0.66666666666666663, '2|high|Cold': 1e-06, '2|normal|Cold': 1.0, '3|yes|Flu': 1.0, '3|no|Flu': 1e-06, '3|yes|Cold': 0.5, '3|no|Cold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Generate prediction for the testSet\n",
    "@param testSet = test data that will be classified\n",
    "@param priorProb, posteriorProb = model\n",
    "@return predictedClasses = array containing prediction made by model\n",
    "'''\n",
    "def predictSup(testSet, priorProb, posteriorProb):\n",
    "    cleanTest = testSet.drop(testSet.columns[-1], axis=1)\n",
    "    predictedClasses = []\n",
    "\n",
    "    for i, instance in cleanTest.iterrows():\n",
    "        currentMax = ['null', -float(\"inf\")]  # track most probable class\n",
    "\n",
    "        for c in priorProb.index:\n",
    "            # maximum likelihood estimation of each instance\n",
    "            prob = np.log(priorProb[c])\n",
    "\n",
    "            for attrib, val in enumerate(list(instance)):\n",
    "                key = createKey([attrib, val, c])\n",
    "                if key in posteriorProb: prob += np.log(posteriorProb[key])\n",
    "\n",
    "            if prob > currentMax[1]: currentMax = [c, prob]\n",
    "\n",
    "        # predicted class = most likely class\n",
    "        predictedClasses.append(currentMax[0])\n",
    "\n",
    "    return predictedClasses\n",
    "# (train, test) = preprocessSup(SAMPLE, True)\n",
    "# (prior, posterior) = trainSup(train)\n",
    "# print(predictSup(test, prior, posterior))\n",
    "# print(test)\n",
    "# print(train)\n",
    "# print(prior)\n",
    "# print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple accuracy measure of the supervised context\n",
    "@param testSet = array of test result\n",
    "@param predictedClasses = array of predicted result\n",
    "@return accuract = (TP+TN) / (TP+TN+FP+FN)\n",
    "'''\n",
    "def evaluateSup(testSet, predictedClasses):\n",
    "    correct = 0\n",
    "    trueClass = testSet.iloc[:,-1].tolist()\n",
    "\n",
    "    if len(trueClass) != len(predictedClasses):\n",
    "        print('Error: Class length')\n",
    "        return\n",
    "\n",
    "    for i in range(len(trueClass)):\n",
    "        if (trueClass[i] == predictedClasses[i]): correct += 1\n",
    "\n",
    "    return correct / len(trueClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create confusion matrix for supervised\n",
    "@param trueClass = true class result array\n",
    "@param predictedClasses = prediction classes array\n",
    "@param classes = possible unique classes\n",
    "@return confusionMatrix = confusion matrix\n",
    "'''\n",
    "def createConfusionMatrixSup(trueClass, predictedClasses, classes):\n",
    "    '''\n",
    "    builds a confusion matrix for unsupervised evaluation\n",
    "    '''\n",
    "    if len(trueClass) != len(predictedClasses):\n",
    "        print('Error: Class length')\n",
    "        return\n",
    "\n",
    "    # Create a pandas dataframe actual is the row, predicted is the column\n",
    "    confusionMatrix = pd.DataFrame()\n",
    "    for c in classes: confusionMatrix[c] = [0] * len(classes)\n",
    "\n",
    "    confusionMatrix.index = classes  # index by classes\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    for i in range(len(trueClass)):\n",
    "        confusionMatrix.loc[trueClass[i], predictedClasses[i]] += 1\n",
    "\n",
    "    # Add actual and predicted labels\n",
    "    predictedCol = []\n",
    "    actualRow = []\n",
    "\n",
    "    for string in classes:\n",
    "        predictedCol.append(string + ' (Predicted)')\n",
    "        actualRow.append(string + ' (Actual)')\n",
    "\n",
    "    confusionMatrix.columns = predictedCol\n",
    "    confusionMatrix.index = actualRow\n",
    "\n",
    "    return(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Initialise instances with random (non-uniform) class distributions\n",
    "'''\n",
    "def trainUnsup(df):\n",
    "    classList = set(df.iloc[:,-1])  # extract unique classes\n",
    "    classCount = len(classList)\n",
    "    cleanTrain = df.drop(df.columns[-1], axis=1)  # drop class col\n",
    "    N = cleanTrain.shape[0]  # instance count\n",
    "    priorCounts = float()  #initialise prior\n",
    "    randoms = []\n",
    "\n",
    "    # initialise class probabilities as float\n",
    "    for c in classList: cleanTrain[c] = float()\n",
    "\n",
    "    # generate N random probability distributions, while summing for prior\n",
    "    # store generated probabilities in dataframe\n",
    "    for i in range(N):\n",
    "        randoms.append(randDistGen(classCount))\n",
    "        priorCounts += randoms[i]\n",
    "\n",
    "        for idx, c in enumerate(classList):\n",
    "            cleanTrain.at[i, c] = randoms[i][idx]\n",
    "\n",
    "    # slide example\n",
    "    # randoms2 = [[ 0.4,  0.6],\n",
    "    #             [ 0.7,  0.3],\n",
    "    #             [ 0.9,  0.1],\n",
    "    #             [ 0.2,  0.8],\n",
    "    #             [ 0.6,  0.4]]\n",
    "    # randoms = randoms2\n",
    "    # priorCounts= np.array([2.8, 2.2])\n",
    "\n",
    "    # print('priorCounts', priorCounts)\n",
    "    # print('priorProb', priorCounts / N)\n",
    "    # print('INIT\\n', cleanTrain)\n",
    "\n",
    "\n",
    "    return cleanTrain, classList, priorCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictUnsup(cleanTrain, classes, priorCounts, trueClass):\n",
    "    '''\n",
    "    returns predicted classes and final class distributions\n",
    "    '''\n",
    "    N = cleanTrain.shape[0]  # instance count\n",
    "    attribCount = cleanTrain.shape[1] - len(classes)\n",
    "    priorProb = priorCounts / N\n",
    "\n",
    "    for j in range(ITERATIONS):\n",
    "\n",
    "        predictedClasses = []\n",
    "        # extract final predictions (most likely class)\n",
    "        for i, instance in cleanTrain.iterrows():\n",
    "            currentMax = ['null', 0]\n",
    "\n",
    "            for idx, c in enumerate(classes):\n",
    "                if instance[c] > currentMax[1]: currentMax = [c, instance[c]]\n",
    "            predictedClasses.append(currentMax[0])\n",
    "\n",
    "        evaluateUnsup(trueClass, predictedClasses, classes, True)\n",
    "\n",
    "        posteriorProb = defaultdict(lambda: 0)\n",
    "\n",
    "        # generate attribute value|class pair probabilities\n",
    "        for attrib in range(attribCount) :\n",
    "            attribValues = list(cleanTrain[attrib].unique())\n",
    "            if ('?' in attribValues): attribValues.remove('?')\n",
    "\n",
    "            for idx, c in enumerate(classes):\n",
    "                for val in attribValues:\n",
    "                    key = createKey([attrib, val, c])\n",
    "                    filterClassVal = cleanTrain[cleanTrain[attrib] == val]\n",
    "                    posteriorProb[key] += filterClassVal[c].sum() / priorCounts[idx]\n",
    "\n",
    "        # maximum likelihood estimation of each instance\n",
    "        for i, instance in cleanTrain.iterrows():\n",
    "            classSum = 0.0\n",
    "\n",
    "            for idx, c in enumerate(classes):\n",
    "                tmpProb = priorProb[idx]\n",
    "\n",
    "                for attrib, val in enumerate(list(instance)):\n",
    "                    key = createKey([attrib, val, c])\n",
    "                    if key in posteriorProb: tmpProb *= posteriorProb[key]\n",
    "                classSum += tmpProb\n",
    "                cleanTrain.at[i, c] = tmpProb\n",
    "\n",
    "            # normalise posterior\n",
    "            for c in classes: cleanTrain.at[i, c] /= classSum\n",
    "\n",
    "        # recalculate prior\n",
    "        for idx,c in enumerate(classes): priorCounts[idx] = cleanTrain[c].sum()\n",
    "        priorProb = priorCounts / N\n",
    "\n",
    "    return predictedClasses, cleanTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateUnsup(trueClass, predictedClasses, classes, flag):\n",
    "    '''\n",
    "    builds a confusion matrix for unsupervised evaluation\n",
    "    '''\n",
    "    if len(trueClass) != len(predictedClasses):\n",
    "        print('Error: Class length')\n",
    "        return\n",
    "\n",
    "    # Create a pandas dataframe actual is the row, predicted is the column\n",
    "    confusionMatrix = pd.DataFrame()\n",
    "    for c in classes: confusionMatrix[c] = [0] * len(classes)\n",
    "\n",
    "    confusionMatrix.index = classes  # index by classes\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    for i in range(len(trueClass)):\n",
    "        confusionMatrix.loc[trueClass[i], predictedClasses[i]] += 1\n",
    "\n",
    "    # Add actual and predicted labels\n",
    "    predictedCol = []\n",
    "    actualRow = []\n",
    "\n",
    "    for string in classes:\n",
    "        predictedCol.append(string + ' (Predicted)')\n",
    "        actualRow.append(string + ' (Actual)')\n",
    "\n",
    "    confusionMatrix.columns = predictedCol\n",
    "    confusionMatrix.index = actualRow\n",
    "\n",
    "    if flag: print(confusionMatrix)\n",
    "\n",
    "    # calculate unsupervised accuracy\n",
    "    maxSum = 0\n",
    "    totalSum = confusionMatrix.values.sum()\n",
    "    # sum rows or columns???\n",
    "    for c in confusionMatrix.columns: maxSum += confusionMatrix[c].max()\n",
    "\n",
    "    return maxSum / totalSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Used mainly in holdout method to average 10 holdout\n",
    "@param func = function that will be run\n",
    "@param desc = description of experiment\n",
    "@param flag = if true not split else split (default = split)\n",
    "@param flag_print = true print, false otherwise\n",
    "'''\n",
    "def sample_experiment(func, desc, flag, flag_print):\n",
    "    RUNS = 10\n",
    "    print(desc)  # description of experiment\n",
    "\n",
    "    for d in DATASETS:\n",
    "        avgMeasure = 0\n",
    "        for i in range(RUNS): avgMeasure += func(d, flag, flag_print)\n",
    "        print('{} | Avg. Measure: {}'.format(d, avgMeasure / RUNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991999015263417"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Main function for supervised to be run across a dataset\n",
    "@param data = dataset used to run\n",
    "@param flag = if true not split else split (default = split)\n",
    "@param flag_print = true print, false otherwise\n",
    "@return accuracy = accuracy of the data\n",
    "'''\n",
    "def mainSup(data, flag=False, flag_print=True):\n",
    "    # If true (don't split), false split\n",
    "    trainSet, testSet = preprocessSup(data, flag)\n",
    "    \n",
    "    priorProb, posteriorProb = trainSup(trainSet)\n",
    "    predictedClasses = predictSup(testSet, priorProb, posteriorProb)\n",
    "    accuracy = evaluateSup(testSet, predictedClasses)\n",
    "    confusion_matrix = createConfusionMatrixSup(testSet.iloc[:,-1].tolist(), predictedClasses, testSet.iloc[:, -1].unique())\n",
    "    \n",
    "    if (flag_print):\n",
    "        display(confusion_matrix)\n",
    "        print(\"\\nThe accuracy for the dataset is {}.\".format(accuracy))\n",
    "    \n",
    "    # Return accuracy\n",
    "    return accuracy\n",
    "# accuracyTester = mainSup(DATASET4, True)\n",
    "# sample_experiment(mainSup, \"Try the holdout\", False, False)\n",
    "# mainSup(DATASET4, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using holdout, averaged over 10 runs\n",
      "2018S1-proj1_data/breast-cancer-dos.csv | Avg. Measure: 0.7527521941965094\n",
      "2018S1-proj1_data/car-dos.csv | Avg. Measure: 0.8519416177177661\n",
      "2018S1-proj1_data/hypothyroid-dos.csv | Avg. Measure: 0.9521784112150685\n",
      "2018S1-proj1_data/mushroom-dos.csv | Avg. Measure: 0.9913746397451153\n",
      "\n",
      "\n",
      "Training in test data\n",
      "2018S1-proj1_data/breast-cancer-dos.csv | Avg. Measure: 0.7552447552447551\n",
      "2018S1-proj1_data/car-dos.csv | Avg. Measure: 0.8738425925925926\n",
      "2018S1-proj1_data/hypothyroid-dos.csv | Avg. Measure: 0.9522605121719886\n",
      "2018S1-proj1_data/mushroom-dos.csv | Avg. Measure: 0.991999015263417\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Used to answer question 3, using holdout and no holdout\n",
    "'''\n",
    "def mainQuestion3():\n",
    "    # Using holdout\n",
    "    sample_experiment(mainSup, \"Using holdout, averaged over 10 runs\", False, False)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Using no holdout\n",
    "    sample_experiment(mainSup, \"Training in test data\", True, False)\n",
    "\n",
    "# mainQuestion3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deltaQuestion6(df, predict):\n",
    "    '''\n",
    "    Calculates how far away probabilistic estimate of true class is from 1.\n",
    "    Assumes probabilistic estimate of true class = highest probability of all classes due to class 'swapping'\n",
    "    '''\n",
    "    deltaSum = 0\n",
    "\n",
    "    # difference (probability) between each predicted class and 1\n",
    "    for i, row in df.iterrows(): deltaSum += abs(1 - row[predict[i]])\n",
    "\n",
    "    return deltaSum / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mainUnsup(data):\n",
    "    '''\n",
    "    execute unsupervised NB across 'data'\n",
    "    '''\n",
    "    df = pd.read_csv(data, header = None)\n",
    "    trueClass = df.iloc[:,-1].tolist()  # extract true classes\n",
    "    cleanTrain, classes, priorCounts = trainUnsup(df)\n",
    "    predictedClasses, finalDf = predictUnsup(cleanTrain, classes, priorCounts, trueClass)\n",
    "    accuracyUnsup = evaluateUnsup(trueClass, predictedClasses, classes, True)\n",
    "    deltaAvg = deltaQuestion6(finalDf, predictedClasses)\n",
    "\n",
    "    print('delta average', deltaAvg)\n",
    "\n",
    "    return accuracyUnsup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
