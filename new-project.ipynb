{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "\n",
    "SPLIT_RATIO = 0.8  # holdout ratio (according to Pareto principle)\n",
    "ITERATIONS = 10  # iterations for unsupervised NB\n",
    "EPSILON = 10**(-6)\n",
    "\n",
    "DATASET1 = '2018S1-proj1_data/breast-cancer-dos.csv'\n",
    "DATASET2 = '2018S1-proj1_data/car-dos.csv'\n",
    "DATASET3 = '2018S1-proj1_data/hypothyroid-dos.csv'\n",
    "DATASET4 = '2018S1-proj1_data/mushroom-dos.csv'\n",
    "DATASETS = [DATASET1, DATASET2, DATASET3, DATASET4]\n",
    "SAMPLE = '2018S1-proj1_data/sample.csv'  # example dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function to create dictionary key given a string\n",
    "@param lst = list of string to combined\n",
    "@return dictionary key for probability (i.e. 0|mild|flu = (0==mild given flu))\n",
    "'''\n",
    "def createKey(lst):\n",
    "    return '|'.join(str(i) for i in lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Random generator\n",
    "@param length = length of the random array\n",
    "@return array containing random number that sums to 1\n",
    "'''\n",
    "def randDistGen(length):\n",
    "    dist = np.random.random(length)\n",
    "    return dist / dist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocessing for supervised to split the data into a training test split\n",
    "@param data = dataset\n",
    "@param flag = True = no split, False = split\n",
    "'''\n",
    "def preprocessSup(data, flag=False):\n",
    "    dataFrame = pd.read_csv(data, header = None)\n",
    "    \n",
    "    if (flag == False):\n",
    "        # Split according to the splitting ratio\n",
    "        split = np.random.rand(len(dataFrame)) < SPLIT_RATIO\n",
    "    \n",
    "        train = dataFrame[split]\n",
    "        test = dataFrame[~split]\n",
    "    else:\n",
    "        train= dataFrame\n",
    "        test= dataFrame\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create supervised Naive Bayes model by returning prior and posterior probability\n",
    "@param trainSet = data that are used for training to generate model\n",
    "@return priobProb, posteriorProb = probability counter\n",
    "'''\n",
    "def trainSup(trainSet):\n",
    "    priorCounts = trainSet.iloc[:,-1].value_counts()\n",
    "    priorProb = priorCounts / trainSet.shape[0]\n",
    "    attribCount = trainSet.shape[1]\n",
    "    posteriorProb = {}\n",
    "\n",
    "    # Iterating over all columns except for the class column\n",
    "    for attrib in range(attribCount - 1):  \n",
    "        # Generate list of unique attribute values and disregard ?\n",
    "        attribValues = list(trainSet[attrib].unique())\n",
    "        if ('?' in attribValues): attribValues.remove('?')\n",
    "\n",
    "        # Calculate posterior probabilities\n",
    "        for c in priorCounts.index:\n",
    "            for val in attribValues:\n",
    "                # first filter by class then by attribute value\n",
    "                filterClass = trainSet[trainSet.iloc[:,-1] == c]\n",
    "                filterClassVal = filterClass[filterClass[attrib] == val]\n",
    "                \n",
    "                # Generate key for dictionary (0|severe|flu means (column 0=severe|flu)\n",
    "                key = createKey([attrib, val, c])\n",
    "                posteriorProb[key] = filterClassVal.shape[0] / priorCounts[c]\n",
    "        \n",
    "    # Iterate every element in dictionary and perform epsilon smoothing\n",
    "    for key, value in posteriorProb.items():\n",
    "        if (value == 0):\n",
    "            posteriorProb[key] = EPSILON\n",
    "    \n",
    "    return priorProb, posteriorProb\n",
    "\n",
    "# (train, test) = preprocessSup(SAMPLE, True)\n",
    "# (prior, posterior) = trainSup(train)\n",
    "# print(prior)\n",
    "# print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate prediction for the testSet\n",
    "@param testSet = test data that will be classified\n",
    "@param priorProb, posteriorProb = model\n",
    "@return predictedClasses = array containing prediction made by model\n",
    "'''\n",
    "def predictSup(testSet, priorProb, posteriorProb):\n",
    "    cleanTest = testSet.drop(testSet.columns[-1], axis=1)\n",
    "    predictedClasses = []\n",
    "\n",
    "    for i, instance in cleanTest.iterrows():\n",
    "        currentMax = ['null', 0]  # track most probable class\n",
    "\n",
    "        for c in priorProb.index:\n",
    "            # maximum likelihood estimation of each instance\n",
    "            prob = priorProb[c]\n",
    "\n",
    "            for attrib, val in enumerate(list(instance)):\n",
    "                key = createKey([attrib, val, c])\n",
    "                if key in posteriorProb: prob += posteriorProb[key]\n",
    "\n",
    "            if prob > currentMax[1]: currentMax = [c, prob]\n",
    "\n",
    "        # predicted class = most likely class\n",
    "        predictedClasses.append(currentMax[0])\n",
    "\n",
    "    return predictedClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple accuracy measure of the supervised context\n",
    "@param testSet = array of test result\n",
    "@param predictedClasses = array of predicted result\n",
    "@return accuract = (TP+TN) / (TP+TN+FP+FN)\n",
    "'''\n",
    "def evaluateSup(testSet, predictedClasses):\n",
    "    correct = 0\n",
    "    trueClass = testSet.iloc[:,-1].tolist()\n",
    "\n",
    "    if len(trueClass) != len(predictedClasses):\n",
    "        print('Error: Class length')\n",
    "        return\n",
    "\n",
    "    for i in range(len(trueClass)):\n",
    "        if (trueClass[i] == predictedClasses[i]): correct += 1\n",
    "\n",
    "    return correct / len(trueClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create confusion matrix for supervised\n",
    "@param trueClass = true class result array\n",
    "@param predictedClasses = prediction classes array\n",
    "@param classes = possible unique classes\n",
    "@return confusionMatrix = confusion matrix\n",
    "'''\n",
    "def createConfusionMatrixSup(trueClass, predictedClasses, classes):\n",
    "    '''\n",
    "    builds a confusion matrix for unsupervised evaluation\n",
    "    '''\n",
    "    if len(trueClass) != len(predictedClasses):\n",
    "        print('Error: Class length')\n",
    "        return\n",
    "\n",
    "    # Create a pandas dataframe actual is the row, predicted is the column\n",
    "    confusionMatrix = pd.DataFrame()\n",
    "    for c in classes: confusionMatrix[c] = [0] * len(classes)\n",
    "\n",
    "    confusionMatrix.index = classes  # index by classes\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    for i in range(len(trueClass)):\n",
    "        confusionMatrix.loc[trueClass[i], predictedClasses[i]] += 1\n",
    "\n",
    "    # Add actual and predicted labels\n",
    "    predictedCol = []\n",
    "    actualRow = []\n",
    "\n",
    "    for string in classes:\n",
    "        predictedCol.append(string + ' (Predicted)')\n",
    "        actualRow.append(string + ' (Actual)')\n",
    "\n",
    "    confusionMatrix.columns = predictedCol\n",
    "    confusionMatrix.index = actualRow\n",
    "\n",
    "    return(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainUnsup(df):\n",
    "    '''\n",
    "    initialise instances with random (non-uniform) class distributions\n",
    "    '''\n",
    "    classList = set(df.iloc[:,-1])  # extract unique classes\n",
    "    classCount = len(classList)\n",
    "    cleanTrain = df.drop(df.columns[-1], axis=1)  # drop class col\n",
    "    N = cleanTrain.shape[0]  # instance count\n",
    "    priorCounts = float()  #initialise prior\n",
    "    randoms = []\n",
    "\n",
    "    # initialise class probabilities as float\n",
    "    for c in classList: cleanTrain[c] = float()\n",
    "\n",
    "    # generate N random probability distributions, while summing for prior\n",
    "    # store generated probabilities in dataframe\n",
    "    for i in range(N):\n",
    "        randoms.append(randDistGen(classCount))\n",
    "        priorCounts += randoms[i]\n",
    "\n",
    "        for idx, c in enumerate(classList):\n",
    "            cleanTrain.at[i, c] = randoms[i][idx]\n",
    "\n",
    "    # slide example\n",
    "    # randoms2 = [[ 0.4,  0.6],\n",
    "    #             [ 0.7,  0.3],\n",
    "    #             [ 0.9,  0.1],\n",
    "    #             [ 0.2,  0.8],\n",
    "    #             [ 0.6,  0.4]]\n",
    "    # randoms = randoms2\n",
    "    # priorCounts= np.array([2.8, 2.2])\n",
    "\n",
    "    # print('priorCounts', priorCounts)\n",
    "    # print('priorProb', priorCounts / N)\n",
    "    # print('INIT\\n', cleanTrain)\n",
    "\n",
    "\n",
    "    return cleanTrain, classList, priorCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictUnsup(cleanTrain, classes, priorCounts, trueClass):\n",
    "    '''\n",
    "    returns predicted classes and final class distributions\n",
    "    '''\n",
    "    N = cleanTrain.shape[0]  # instance count\n",
    "    attribCount = cleanTrain.shape[1] - len(classes)\n",
    "    priorProb = priorCounts / N\n",
    "\n",
    "    for j in range(ITERATIONS):\n",
    "\n",
    "        predictedClasses = []\n",
    "        # extract final predictions (most likely class)\n",
    "        for i, instance in cleanTrain.iterrows():\n",
    "            currentMax = ['null', 0]\n",
    "\n",
    "            for idx, c in enumerate(classes):\n",
    "                if instance[c] > currentMax[1]: currentMax = [c, instance[c]]\n",
    "            predictedClasses.append(currentMax[0])\n",
    "\n",
    "        evaluateUnsup(trueClass, predictedClasses, classes, True)\n",
    "\n",
    "        posteriorProb = defaultdict(lambda: 0)\n",
    "\n",
    "        # generate attribute value|class pair probabilities\n",
    "        for attrib in range(attribCount) :\n",
    "            attribValues = list(cleanTrain[attrib].unique())\n",
    "            if ('?' in attribValues): attribValues.remove('?')\n",
    "\n",
    "            for idx, c in enumerate(classes):\n",
    "                for val in attribValues:\n",
    "                    key = createKey([attrib, val, c])\n",
    "                    filterClassVal = cleanTrain[cleanTrain[attrib] == val]\n",
    "                    posteriorProb[key] += filterClassVal[c].sum() / priorCounts[idx]\n",
    "\n",
    "        # maximum likelihood estimation of each instance\n",
    "        for i, instance in cleanTrain.iterrows():\n",
    "            classSum = 0.0\n",
    "\n",
    "            for idx, c in enumerate(classes):\n",
    "                tmpProb = priorProb[idx]\n",
    "\n",
    "                for attrib, val in enumerate(list(instance)):\n",
    "                    key = createKey([attrib, val, c])\n",
    "                    if key in posteriorProb: tmpProb *= posteriorProb[key]\n",
    "                classSum += tmpProb\n",
    "                cleanTrain.at[i, c] = tmpProb\n",
    "\n",
    "            # normalise posterior\n",
    "            for c in classes: cleanTrain.at[i, c] /= classSum\n",
    "\n",
    "        # recalculate prior\n",
    "        for idx,c in enumerate(classes): priorCounts[idx] = cleanTrain[c].sum()\n",
    "        priorProb = priorCounts / N\n",
    "\n",
    "\n",
    "\n",
    "    return predictedClasses, cleanTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateUnsup(trueClass, predictedClasses, classes, flag):\n",
    "    '''\n",
    "    builds a confusion matrix for unsupervised evaluation\n",
    "    '''\n",
    "    if len(trueClass) != len(predictedClasses):\n",
    "        print('Error: Class length')\n",
    "        return\n",
    "\n",
    "    # Create a pandas dataframe actual is the row, predicted is the column\n",
    "    confusionMatrix = pd.DataFrame()\n",
    "    for c in classes: confusionMatrix[c] = [0] * len(classes)\n",
    "\n",
    "    confusionMatrix.index = classes  # index by classes\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    for i in range(len(trueClass)):\n",
    "        confusionMatrix.loc[trueClass[i], predictedClasses[i]] += 1\n",
    "\n",
    "    # Add actual and predicted labels\n",
    "    predictedCol = []\n",
    "    actualRow = []\n",
    "\n",
    "    for string in classes:\n",
    "        predictedCol.append(string + ' (Predicted)')\n",
    "        actualRow.append(string + ' (Actual)')\n",
    "\n",
    "    confusionMatrix.columns = predictedCol\n",
    "    confusionMatrix.index = actualRow\n",
    "\n",
    "    if flag: print(confusionMatrix)\n",
    "\n",
    "    # calculate unsupervised accuracy\n",
    "    maxSum = 0\n",
    "    totalSum = confusionMatrix.values.sum()\n",
    "    # sum rows or columns???\n",
    "    for c in confusionMatrix.columns: maxSum += confusionMatrix[c].max()\n",
    "\n",
    "    return maxSum / totalSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Used mainly in holdout method to average 10 holdout\n",
    "@param func = function that will be run\n",
    "@param desc = description of experiment\n",
    "'''\n",
    "def sample(func, desc):\n",
    "    RUNS = 10\n",
    "    print(desc)  # description of experiment\n",
    "\n",
    "    for d in DATASETS:\n",
    "        avgMeasure = 0\n",
    "        for i in range(RUNS): avgMeasure += func(d)\n",
    "        print('{} | Avg. Measure: {}'.format(d, avgMeasure / RUNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main function for supervised to be run across a dataset\n",
    "@param data = dataset used to run\n",
    "@return accuracy = accuracy of the data\n",
    "'''\n",
    "def mainSup(data):\n",
    "    # If true (don't split), false split\n",
    "    trainSet, testSet = preprocessSup(data, True)\n",
    "    \n",
    "    priorProb, posteriorProb = trainSup(trainSet)\n",
    "    predictedClasses = predictSup(testSet, priorProb, posteriorProb)\n",
    "    accuracy = evaluateSup(testSet, predictedClasses)\n",
    "    confusion_matrix = createConfusionMatrixSup(testSet.iloc[:,-1].tolist(), predictedClasses, testSet.iloc[:, -1].unique())\n",
    "    \n",
    "    display(confusion_matrix)\n",
    "    print(\"\\nThe accuracy for the dataset is {}.\".format(accuracy))\n",
    "    \n",
    "    # Return accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mainQuestion3(data):\n",
    "    '''\n",
    "    test on training data for Question 3 (no holdout, supervised)\n",
    "    '''\n",
    "    df = pd.read_csv(data, header = None)\n",
    "    priorProb, posteriorProb = trainSup(df)\n",
    "    predictedClasses = predictSup(df, priorProb, posteriorProb)\n",
    "    accuracy = evaluateSup(df, predictedClasses)\n",
    "    #print('Dataset: {}, Accuracy: {}'.format(data, accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deltaQuestion6(df, predict):\n",
    "    '''\n",
    "    Calculates how far away probabilistic estimate of true class is from 1.\n",
    "    Assumes probabilistic estimate of true class = highest probability of all classes due to class 'swapping'\n",
    "    '''\n",
    "    deltaSum = 0\n",
    "\n",
    "    # difference (probability) between each predicted class and 1\n",
    "    for i, row in df.iterrows(): deltaSum += abs(1 - row[predict[i]])\n",
    "\n",
    "    return deltaSum / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mainUnsup(data):\n",
    "    '''\n",
    "    execute unsupervised NB across 'data'\n",
    "    '''\n",
    "    df = pd.read_csv(data, header = None)\n",
    "    trueClass = df.iloc[:,-1].tolist()  # extract true classes\n",
    "    cleanTrain, classes, priorCounts = trainUnsup(df)\n",
    "    predictedClasses, finalDf = predictUnsup(cleanTrain, classes, priorCounts, trueClass)\n",
    "    accuracyUnsup = evaluateUnsup(trueClass, predictedClasses, classes, True)\n",
    "    deltaAvg = deltaQuestion6(finalDf, predictedClasses)\n",
    "\n",
    "    print('delta average', deltaAvg)\n",
    "\n",
    "    return accuracyUnsup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
